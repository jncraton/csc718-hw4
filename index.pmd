% Homework 4 MPI, OpenMP, and GPU Programming Assignments
% Jon Craton
% December 10th, 2019

Problem 1
=========

> (10 points) What are the strengths and limitations of GPU programing?

GPU programming is very well suited for large numbers of floating point operations that can be executed in parallel. Historically, GPUs were design for rasterization of a large number of polygons into a frame of pixels to be displayed on screen. This process is inherently parallel and uses floating point operations exclusively, so GPUs have been optimized with this workload in mind.

GPUs are not well suited for calculations involving a lot of branching, and may not be as well suited for integer operations. If code makes heavy use of integer operations or branching it may not be well suited to run on a GPU or may need to be reworked significantly before it can run effectively.

One of the most significant downsides of GPU compute is the inherent limitation of the memory model. GPUs have their own dedicated RAM, and in order to operate on data we must first transfer it to the GPU and then transfer results back to the CPU. This can eliminate the benefit of GPU programming for certain workloads, and makes the task of writing efficient code for GPU much more complicated.

GPUs provide incredible numbers of raw floating point operations per second (FLOPS). My CPU can perform a few hundred GFLOPs in ideal conditions, while my GPU is cabable of over an order of magnitude more by computing at several TFLOPs.

Part of the reason that GPUs are able to achieve this speed is by using a massively parallel architecture. GPUs may have thousands of compute nodes, while CPUs almost exclusively contain fewer than 100. In addition, GPU compute makes extensive use of vector instructions to operate on many values at once. Modern x86 CPUs are limited to operating on 16 values at once due to the size of cache lines and limitations of the AVX-512 instruction set. GPUs do not share this limitation and can operate on many data values at once.

Problem 2
=========

> (15 points) Among the four parallel frameworks, multithreaded programming, MPI programming, openMP programming, and GPU programming (e.g., CUDA), we discussed in the class, what will be the strategies you are going to use in general when selecting a parallel framework for your applications?

For me, the first step will be thinking about what the workload looks like for an application. I will first profile or reason about the application to determine where most of the work is being done. I will then consider these sections of code to determine if they can be run in parallel effectively.

If they can be run in parallel, I'll think about several other considerations:

1. How much is shared memory a requirement? If all data is dependent on all other data, multi-machine NUMA architectures using MPI may not be effective.
2. Is my workload mostly floating point with few branches? If not, GPUs may not be especially helpful. GPUs are best suited to problems where we can offload a massive number of paralell floating point operations.
3. What compute resources are available? If I don't have a cluster at my disposal, the benefits of MPI are limited compared to shared memory architecures.
4. Do I need total control? I'd favor using OpenMP over direct multithreaded programming unless I have a compelling reason to need that level of control.

In essense, here's what I'd do:

1. Look for time consuming sections of code that can be run in parallel. If they don't exist, wait a long time for execution or work on a more tractable problem.
2. Favor OpenMP over raw threaded programming. OpenMP is relatively quick and easy and can improve speed significantly.
3. If I have a cluster available, and my workload is ammenable to NUMA, try OpenMPI.
4. If I have a GPU available, and my workload is mostly parallel floating point operations, try GPU programming.

These can all be combined. A modern supercomputer will contain thousands of nodes, each with GPUs, so we'll likely need to employ multiprocessor message passing and GPU programming to fully optimize performance.

Problem 3
=========

> (10 points) Benchmarking of a sequential program reveals that 95 percent of the execution time is spent inside functions that are amendable to parallelization. What is the maximum speed up we could expect from executing a parallel version of this program on 10 processors?

We know that there is 5% of the program that we cannot improve via parallelism. This tells us immediately that a 20x speedup is the most we could expect using infinite processors. Let's calculate what we could get with 10.

Let's build a function with some unit tests to calculate this for us:

```python
def calc_speedup(sequential, parallel, num_threads):
  """
  Calculates possible speedup from parallelizing section using `num_threads`

  >>> calc_speedup(1.0, 0.0, 10000)
  1.0

  >>> calc_speedup(0.0, 1.0, 10000)
  10000.0

  >>> calc_speedup(0.5, 0.5, 4)
  1.6
  """

  new_runtime = sequential + parallel / num_threads

  return 1.0/new_runtime
```

With that implemented, we can simply calculate our result:

```python
calc_speedup(0.05, 0.95, 10)
```

We can anticipate a speedup of less than 7x.

Problem 4
=========

> (65 points) Programming Assignment: A small college wishes to assign unique identification numbers to all of its present and future students. The administration is thinking of using a given various constraints that have been placed on what is considered to be an “acceptable” identifier. The “acceptable” identifier must meet the following constraints:

- The identifier is a six-digit combination of the numerals 0-9
- The first digit may not be a 0.
- Two consecutive digits may not be the same.
- The sum of the digits may not be 7, 11, or 13.

All code for this problem is located in the `src` directory. It can be built and run using the included makefile.

Part 1
------

> (15 points) Write a sequential program to count the different identifiers given these constraints.

The average runtime for the sequential version of the program was 0.059 seconds per iteration.

This program can be found in `src/seq.c`. It is also included inline here:

```c
#include <stdio.h>
#include <stdlib.h>
#include <assert.h>

int is_valid(int num) {
  int sum = 0;
  char buf[10];
  sprintf(buf, "%d", num);

  for (int i = 0; i < 6; i++) {
    if (i > 0 && buf[i-1] == buf[i]) { return 0; }
    sum += buf[i] - 48;
  }

  return (sum != 7 && sum != 11 && sum != 13);  
}

void test() {
  // Repetition tests
  assert(!is_valid(999999));
  assert(!is_valid(123455));
  assert(!is_valid(112345));
  assert(!is_valid(123345));
  assert(is_valid(123456));
  assert(is_valid(987565));

  // Sum tests
  assert(is_valid(101010));
  assert(is_valid(123456));
  assert(is_valid(869205));
  assert(!is_valid(101032));
  assert(!is_valid(205202));
  assert(!is_valid(202016));
  assert(!is_valid(202018));
  assert(!is_valid(620320));
}

int main() {
  int i, sum, count;

  test();

  /*
  1. The identifier is a six-digit combination of the numerals 0-9
  2. The first digit may not be a 0.
  3. Two consecutive digits may not be the same.
  4. The sum of the digits may not be 7, 11, or 13.
  */

  for (int runs=0; runs < 100; runs++) {
    sum=0;
    // Loop conditions enforce the first two requirements
    for (i = 100000; i <= 999999; i++) {
      if (is_valid(i)) {
        sum++;
      }
    }
  }

  printf("Sum: %d\n", sum);
}
```

Part 2
------

> (25 points) Convert the sequential program to an MPI parallel program.

This version of the program runs in .029 seconds per iteration using 4 processes. This is nearly double the single processor performance.

The code for this located in `src/mpi.c`. It is also included inline here:

```c
#include <stdio.h>
#include <stdlib.h>
#include <assert.h>
#include <mpi.h>

int is_valid(int num) {
  int sum = 0;
  char buf[10];
  sprintf(buf, "%d", num);

  for (int i = 0; i < 6; i++) {
    if (i > 0 && buf[i-1] == buf[i]) { return 0; }
    sum += buf[i] - 48;
  }

  return (sum != 7 && sum != 11 && sum != 13);  
}

void test() {
  // Repetition tests
  assert(!is_valid(999999));
  assert(!is_valid(123455));
  assert(!is_valid(112345));
  assert(!is_valid(123345));
  assert(is_valid(123456));
  assert(is_valid(987565));

  // Sum tests
  assert(is_valid(101010));
  assert(is_valid(123456));
  assert(is_valid(869205));
  assert(!is_valid(101032));
  assert(!is_valid(205202));
  assert(!is_valid(202016));
  assert(!is_valid(202018));
  assert(!is_valid(620320));
}

int main(int argc, char **argv) {
  int id, i, count=0, global_count, num_procs;

  test();

  MPI_Init (&argc, &argv);
  MPI_Barrier(MPI_COMM_WORLD);

  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
  MPI_Comm_rank (MPI_COMM_WORLD, &id);

  /*
  1. The identifier is a six-digit combination of the numerals 0-9
  2. The first digit may not be a 0.
  3. Two consecutive digits may not be the same.
  4. The sum of the digits may not be 7, 11, or 13.
  */

  for (int runs=0; runs < 100; runs++) {
    count=0;
    // Loop conditions enforce the first two requirements
    for (i = 100000+id; i <= 999999; i+=num_procs) {
      if (is_valid(i)) {
        count++;
      }
    }
  }

  MPI_Reduce (&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
  MPI_Finalize();

  if (!id) printf("Count: %d\n", global_count);
}
```

Part 3
------

> (25 points) (choose only one: A or B, not both)
> 
> A. Convert the sequential program to an openMP parallel program.
> 
> B. Convert the sequential program to a GPU program using a parallel framework at your choice (e.g., CUDA, OpenCL, etc.)

The openMP implementation is available in `src/omp.c`. It takes an average of 0.014 seconds per iteration using 4 threads approaching a 4x improvement over the sequential version.

The source is included inline here:

```c
#include <stdio.h>
#include <stdlib.h>
#include <assert.h>
#include <omp.h>

int is_valid(int num) {
  int sum = 0;
  char buf[10];
  sprintf(buf, "%d", num);

  for (int i = 0; i < 6; i++) {
    if (i > 0 && buf[i-1] == buf[i]) { return 0; }
    sum += buf[i] - 48;
  }

  return (sum != 7 && sum != 11 && sum != 13);  
}

void test() {
  // Repetition tests
  assert(!is_valid(999999));
  assert(!is_valid(123455));
  assert(!is_valid(112345));
  assert(!is_valid(123345));
  assert(is_valid(123456));
  assert(is_valid(987565));

  // Sum tests
  assert(is_valid(101010));
  assert(is_valid(123456));
  assert(is_valid(869205));
  assert(!is_valid(101032));
  assert(!is_valid(205202));
  assert(!is_valid(202016));
  assert(!is_valid(202018));
  assert(!is_valid(620320));
}

int main() {
  int i, sum=0, count;

  test();

  /*
  1. The identifier is a six-digit combination of the numerals 0-9
  2. The first digit may not be a 0.
  3. Two consecutive digits may not be the same.
  4. The sum of the digits may not be 7, 11, or 13.
  */

  omp_set_num_threads(4);

  for (int runs=0; runs < 100; runs++) {
    sum=0;
    // Loop conditions enforce the first two requirements
    #pragma omp parallel for private(i) reduction(+: sum)
    for (i = 100000; i <= 999999; i++) {
      if (is_valid(i)) {
        sum++;
      }
    }
  }

  printf("Sum: %d\n", sum);
}
```

Due: Dec 10, by Midnight 2019

Total: 100 points

Note: For all the programming assignments, you can choose any operating
systems you want. I will usually provide C/C++ samples for the
programming assignments. If you prefer to use other languages, e.g.,
Java, they are accepted too. A README.txt is required to submit any
programming assignments. In the README.txt, you need to provide the
following information:

1)  How to compile your program?

2)  How to run your program?

3)  What is the average running time of your program?

4)  What are the expected output results when I run your program?

5)  Any descriptions which may help me to compile, run, and verify your
    answers. (FYI: I check every programming assignment turned in!)

Zip all you source code, project files, supporting files, and README.txt
and submit the all-in-one zip file together to the D2L Dropbox. If you
have any questions about the homework, please let me know.
